nameOverride: ""
fullNameOverride: ""

# LONG_RANDOM_STRING -- A long random string. You should probably provide your own. This is needed for OIDC.
LONG_RANDOM_STRING: 1234

nodeSelector: {}
affinity: {}
tolerations: []
podAnnotations: {}

# Configuration related to the search service.
search:
  # search.serviceName -- The search service name.
  serviceName: search

  # search.serviceType -- The search service type. See service types [ref](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
  serviceType: ClusterIP

  # search.elasticsearchEndpoint -- The name of the service hosting elasticsearch on your cluster, if you bring your own. You should only need to change this, if you don't use the version in this chart.
  elasticsearchEndpoint:

  # search.image -- The image of the search container.
  image: amundsendev/amundsen-search

  # search.imageTag -- The image tag of the search container.
  imageTag: 2.4.0

  # search.replicas -- How many replicas of the search service to run.
  replicas: 1

  # search.resources -- See pod resourcing [ref](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  #  requests:
  #    cpu: 1
  #    memory: 1Gi

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

##
# Configuration related to the metadata service.
##
metadata:
  # metadata.serviceName -- The metadata service name.
  serviceName: metadata

  # metadata.serviceType -- The metadata service type. See service types [ref](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
  serviceType: ClusterIP

  # metadata.neo4jEndpoint -- The name of the service hosting neo4j on your cluster, if you bring your own. You should only need to change this, if you don't use the version in this chart.
  neo4jEndpoint:

  # metadata.image -- The image of the metadata container.
  image: amundsendev/amundsen-metadata

  # metadata.imageTag -- The image tag of the metadata container.
  imageTag: 2.5.4

  # metadata.replicas -- How many replicas of the metadata service to run.
  replicas: 1

  # metadata.resources -- See pod resourcing [ref](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  #  requests:
  #    cpu: 1
  #    memory: 1Gi

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

##
# Configuration related to the frontEnd service.
##
frontEnd:
  # frontEnd.serviceName -- The frontend service name.
  serviceName: frontend

  # frontEnd.serviceType -- The frontend service type. See service types [ref](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
  serviceType: ClusterIP

  # frontEnd.image -- The image of the frontend container.
  image: amundsendev/amundsen-frontend

  # frontEnd.imageTag -- The image tag of the frontend container.
  imageTag: 2.1.1

  # frontEnd.servicePort -- The port the frontend service will be exposed on via the loadbalancer.
  servicePort: 80

  # frontEnd.replicas -- How many replicas of the frontend service to run.
  replicas: 1

  # frontEnd.baseUrl -- used by notifications util to provide links to amundsen pages in emails.
  baseUrl: http://localhost

  # frontEnd.oidcEnabled -- To enable auth via OIDC, set this to true.
  oidcEnabled: false

  # frontEnd.createOidcSecret -- OIDC needs some configuration. If you want the chart to make your secrets, set this to true and set the next four values. If you don't want to configure your secrets via helm, you can still use the amundsen-oidc-config.yaml as a template
  createOidcSecret: false

  # frontEnd.env --  Environments
  env:
    # frontEnd.env.OIDC_CLIENT_ID -- The client id for OIDC.
    OIDC_CLIENT_ID:
    # frontEnd.env.OIDC_CLIENT_SECRET -- The client secret for OIDC.
    OIDC_CLIENT_SECRET: ""
    # frontEnd.env.OIDC_ORG_URL -- The organization URL for OIDC.
    OIDC_ORG_URL:
    # frontEnd.env.OIDC_AUTH_SERVER_ID -- The authorization server id for OIDC.
    OIDC_AUTH_SERVER_ID:
    # frontEnd.env.PREVIEW_DATA_DB_CONN_STRING
    PREVIEW_DATA_DB_CONN_STRING:
    # frontEnd.env.PREVIEW_DATA_STORAGE_SCHEMA
    PREVIEW_DATA_STORAGE_SCHEMA:
    # frontEnd.env.PREVIEW_DATA_STORAGE_TABLE_NAME
    PREVIEW_DATA_STORAGE_TABLE_NAME:

  # Name of a ConfigMap containing extra env vars
  ##
  extraEnvVarsCM:
  # Name of a Secret containing extra env vars
  ##
  extraEnvVarsSecret:

  # frontEnd.resources -- See pod resourcing [ref](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/)
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  #  requests:
  #    cpu: 1
  #    memory: 1Gi

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

  ingress:
    enabled: false
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - frontend.example.com
    tls: []
    #  - secretName:frontend.example.com
    #    hosts:
    #      - frontend.example.com

##
# Configuration related to neo4j.
# https://github.com/neo4j-contrib/neo4j-helm/blob/master/values.yaml
##
neo4j:
  # neo4j.enabled
  enabled: true

  # neo4j.neo4jEndpoint -- Use external neo4j endpoint
  neo4jEndpoint:

  # neo4j.version -- The neo4j application version used by amundsen.
  version: 3.3.0

  # Use password authentication
  authEnabled: false

  # Specify password for neo4j user
  # Defaults to a random 10-character alphanumeric string if not set and authEnabled is true
  # neo4jPassword:

  # neo4j.resources
  resources: {}
  #  limits:
  #    cpu: 2
  #    memory: 2Gi
  # requests:
  #    cpu: 1
  #    memory: 1Gi

  # neo4j.config -- Neo4j application specific configuration. This type of configuration is why the charts/stable version is not used. See [ref](https://github.com/helm/charts/issues/21439)
  config:
    # neo4j.config.dbms -- dbms config for neo4j
    dbms:
      # neo4j.config.dbms.heap_initial_size -- the initial java heap for neo4j
      heap_initial_size: 1G
      # neo4j.config.dbms.heap_max_size -- the max java heap for neo4j
      heap_max_size: 2G
      # neo4j.config.dbms.pagecache_size -- the page cache size for neo4j
      pagecache_size: 2G

  # neo4j.persistence -- Neo4j persistence. Turn this on to keep your data between pod crashes, etc. This is also needed for backups.
  persistence: {}
  #  storageClass: gp2
  #  size: 10Gi
  #  accessMode: ReadWriteMany
  #  efs:
  #    dns:

  # neo4j.backup -- If enabled is set to true, make sure and set the s3 path as well.
  backup:
    # neo4j.backup.enabled - Whether to include the backup neo4j cron pod. If set to true, s3Path is required.
    enabled: false
    # neo4j.backup.s3Path -- The s3path to write to for backups.
    s3Path: "s3://dev/null"
    # neo4j.backup.schedule -- The schedule to run backups on. Defaults to hourly.
    schedule: "0 * * * *"
    podAnnotations: {}

  nodeSelector: {}
  affinity: {}
  tolerations: []
  annotations: {}
  podAnnotations: {}

#
# Configuration related to elasticsearch.
#
# To add values to dependent charts, prefix the value with the chart name (e.g. elasticsearch)
# By default, the ES chart runs with 3,3,2 nodes for master, data, client. Amundsen likely does not need so much,
# so, this has been tuned down to 1,1,1.
#
elasticsearch:
  # elasticsearch.enabled -- set this to false, if you want to provide your own ES instance.
  enabled: true
  image: docker.elastic.co/elasticsearch/elasticsearch-oss
  cluster:
    env:
      # elasticsearch.cluster.env.MINIMUM_MASTER_NODES -- required to match master.replicas
      MINIMUM_MASTER_NODES: 1
      # elasticsearch.cluster.env.EXPECTED_MASTER_NODES -- required to match master.replicas
      EXPECTED_MASTER_NODES: 1
      # elasticsearch.cluster.env.RECOVER_AFTER_MASTER_NODES -- required to match master.replicas
      RECOVER_AFTER_MASTER_NODES: 1
  master:
    # elasticsearch.master.replicas -- only running amundsen on 1 master replica
    replicas: 1
    persistence:
      enabled: false
  data:
    # elasticsearch.data.replicas -- only running amundsen on 1 data replica
    replicas: 1
    persistence:
      enabled: false
  client:
    # elasticsearch.client.replicas -- only running amundsen on 1 client replica
    replicas: 1
  #  serviceType: LoadBalancer
  #  serviceAnnotations:
  #    external-dns.alpha.kubernetes.io/hostname: amundsen-elasticsearch.dev.teamname.company.com
  #    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
  #    service.beta.kubernetes.io/aws-load-balancer-type: nlb
  #  nodeAffinity: high
  #  resources:
  #    limits:
  #      cpu: 2
  #      memory: 2Gi
